# Multi-Modal Candidate Shortlisting System
---
Objective
Develop an intelligent system that can assess job candidates using both their resume text and structured profile data to predict whether they should be shortlisted for an interview.

---
##  Dataset Generation

Since no suitable public dataset existed, I generated a synthetic one using `Faker` and custom templates.

### Steps:
- Created realistic **resumes** using $100$ templated text(generated by gemini) with injected `skills`, `experience`, etc.
- Constructed structured fields: `degree`, `GPA`, `age`, `years_of_experience`, etc.
- **Labels** were generated using a rule-based logic:
  - Candidates with good GPA, enough experience, and presence of key skills in resume → `shortlisted = 1`
  - Others → `shortlisted = 0`
- **Skills were removed from tabular input**, so the model relies on BERT to extract them from resume text.

### Sample Output:
Each candidate has:
```json
{
  "resume": "John is a B.Tech graduate with experience in Python, SQL...",
  "degree": "B.Tech",
  "field_of_study": "Computer Science",
  "GPA": 8.5,
  "years_of_experience": 2.5,
  "age": 24,
  "shortlisted": 1
}

---
### **MultimodalResumeClassifier**

Input Layer
├── Text Branch (BERT)
│   ├── BERT Tokenizer (max_length=256)
│   ├── bert-base-uncased (768 dim output)
│   └── CLS token pooled output
│
├── Tabular Branch (Neural Network)
│   ├── Input: [degree_encoded, field_encoded, GPA_scaled, exp_scaled, age_scaled]
│   ├── Dense(128) → BatchNorm → ReLU → Dropout(0.3)
│   └── Dense(64) → BatchNorm → ReLU → Dropout(0.3)
│
└── Fusion Layer
    ├── Concatenate(text_features[768] + tabular_features[64])
    ├── Dense(256) → BatchNorm → ReLU → Dropout(0.3)
    ├── Dense(128) → BatchNorm → ReLU → Dropout(0.3)
    ├── Dense(64) → ReLU → Dropout(0.3)
    └── Dense(1) → Sigmoid (probability output)
```

### **Key Design Decisions:**
- **Early Fusion**: Combines modalities before final layers for better interaction learning
- **Batch Normalization**: Stabilizes training and improves convergence
- **Dropout Regularization**: Prevents overfitting (30% dropout rate)
- **Skip Connections**: Could be added for deeper networks (future enhancement)

---
## **Data Preprocessing Pipeline**

### **1. Text Processing**
```python
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
encoded = tokenizer.encode_plus(
    text,
    add_special_tokens=True,    # [CLS] and [SEP] tokens
    max_length=256,             # Optimal length for resumes
    padding='max_length',       # Uniform input size
    truncation=True,            # Handle long resumes
    return_attention_mask=True  # Ignore padding tokens
)
```

### **2. Tabular Feature Engineering**
```python
# Categorical Encoding
label_encoders = {}
for col in ['degree', 'field_of_study']:
    le = LabelEncoder()
    df[col + '_encoded'] = le.fit_transform(df[col])

# Numerical Scaling
scalers = {}
for col in ['GPA', 'years_of_experience', 'age']:
    scaler = StandardScaler()
    df[col + '_scaled'] = scaler.fit_transform(df[[col]])
```

### **3. Stratified Data Splitting**
- **Train**: 70% (maintains class distribution)
- **Validation**: 15% (for hyperparameter tuning)
- **Test**: 15% (final evaluation)

---

## **Training Configuration**

### **Hyperparameters**
```python
LEARNING_RATE = 2e-5          # Optimal for BERT fine-tuning
BATCH_SIZE = 16               # Memory-efficient training
EPOCHS = 25                   # With early stopping
WEIGHT_DECAY = 0.01           # L2 regularization
DROPOUT_RATE = 0.3            # Aggressive regularization
GRADIENT_CLIP = 1.0           # Prevent exploding gradients
```

### **Optimization Strategy**
- **AdamW Optimizer**: Better weight decay handling than Adam
- **Cosine Annealing**: Smooth learning rate decay
- **Early Stopping**: Patience=5, prevents overfitting
- **Gradient Clipping**: Stabilizes training dynamics

---

##  **Results & Performance**

### **Final Metrics**
```
Test Accuracy:  79.33%
Test Precision: 82.15%
Test Recall:    75.68%
Test F1-Score:  78.79%
Test AUC:       85.42%
```

### **Training Progress**
- **Best Epoch**: 21/25 (Early stopping triggered)
- **Training Accuracy**: 88.29%
- **Validation Accuracy**: 79.33%

### **Model Insights**
- **Text Branch Contribution**: ~65% of decision weight
- **Tabular Branch Contribution**: ~35% of decision weight
- **Most Important Features**: 
  1. Skills mentioned in resume (from BERT)
  2. Years of experience
  3. GPA score
  4. Educational background

---

##  **Usage**

##  **Usage**

### **Installation**
```bash
pip install torch transformers scikit-learn pandas numpy matplotlib tqdm faker
```

### **Model Download**
Due to the large model size (450MB+), the trained model is hosted on Google Drive:

**Download Link**: [Resume classifying model](https://drive.google.com/file/d/1srHUkGgFkiAHZsqm3pe3a7zrxTzhFj8g/view?usp=sharing)

Download the `best_model.pth` file and place it in your project directory.

---